{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\atlas-weather\\atlas.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39m# Before start you should be to install JAVA\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000000?line=3'>4</a>\u001b[0m spark \u001b[39m=\u001b[39m (SparkSession\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39m.\u001b[39;49mbuilder\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000000?line=5'>6</a>\u001b[0m \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mWeather-Atlas\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39m.\u001b[39;49mgetOrCreate())\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=225'>226</a>\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=226'>227</a>\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=227'>228</a>\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=228'>229</a>\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=229'>230</a>\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=230'>231</a>\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=389'>390</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=390'>391</a>\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=391'>392</a>\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=392'>393</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=138'>139</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=139'>140</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=140'>141</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=141'>142</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=143'>144</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=144'>145</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=146'>147</a>\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=336'>337</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=337'>338</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=338'>339</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=339'>340</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=341'>342</a>\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=104'>105</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=107'>108</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=109'>110</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=110'>111</a>\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Before start you should be to install JAVA\n",
    "spark = (SparkSession\n",
    ".builder\n",
    ".appName(\"Weather-Atlas\")\n",
    ".getOrCreate()) # Entry-point to PySpark App\n",
    "\n",
    "# If click A- [above] or B - [below] in VSC hepls to add code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Formatted Date: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Precip Type: string (nullable = true)\n",
      " |-- Temperature (C): string (nullable = true)\n",
      " |-- Apparent Temperature (C): string (nullable = true)\n",
      " |-- Humidity: string (nullable = true)\n",
      " |-- Wind Speed (km/h): string (nullable = true)\n",
      " |-- Wind Bearing (degrees): string (nullable = true)\n",
      " |-- Visibility (km): string (nullable = true)\n",
      " |-- Loud Cover: string (nullable = true)\n",
      " |-- Pressure (millibars): string (nullable = true)\n",
      " |-- Daily Summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read of DataFrame with auto identification of schema\n",
    "df = spark.read.format(\"csv\").option('header', \"true\").load(\"../dataset/weatherHistory.csv\")\n",
    "\n",
    "df.printSchema() # Help to explore schema of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+-----------------+------------------------+--------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
      "|      Formatted Date|      Summary|Precip Type|  Temperature (C)|Apparent Temperature (C)|Humidity| Wind Speed (km/h)|Wind Bearing (degrees)|   Visibility (km)|Loud Cover|Pressure (millibars)|       Daily Summary|\n",
      "+--------------------+-------------+-----------+-----------------+------------------------+--------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
      "|2006-04-01 00:00:...|Partly Cloudy|       rain|9.472222222222221|      7.3888888888888875|    0.89|           14.1197|                 251.0|15.826300000000002|       0.0|             1015.13|Partly cloudy thr...|\n",
      "|2006-04-01 01:00:...|Partly Cloudy|       rain|9.355555555555558|       7.227777777777776|    0.86|           14.2646|                 259.0|15.826300000000002|       0.0|             1015.63|Partly cloudy thr...|\n",
      "|2006-04-01 02:00:...|Mostly Cloudy|       rain|9.377777777777778|       9.377777777777778|    0.89|3.9284000000000003|                 204.0|           14.9569|       0.0|             1015.94|Partly cloudy thr...|\n",
      "|2006-04-01 03:00:...|Partly Cloudy|       rain| 8.28888888888889|       5.944444444444446|    0.83|           14.1036|                 269.0|15.826300000000002|       0.0|             1016.41|Partly cloudy thr...|\n",
      "|2006-04-01 04:00:...|Mostly Cloudy|       rain|8.755555555555553|       6.977777777777779|    0.83|           11.0446|                 259.0|15.826300000000002|       0.0|             1016.51|Partly cloudy thr...|\n",
      "|2006-04-01 05:00:...|Partly Cloudy|       rain|9.222222222222221|        7.11111111111111|    0.85|           13.9587|                 258.0|           14.9569|       0.0|             1016.66|Partly cloudy thr...|\n",
      "|2006-04-01 06:00:...|Partly Cloudy|       rain|7.733333333333334|       5.522222222222221|    0.95|           12.3648|                 259.0| 9.982000000000001|       0.0|             1016.72|Partly cloudy thr...|\n",
      "+--------------------+-------------+-----------+-----------------+------------------------+--------+------------------+----------------------+------------------+----------+--------------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploration of WEATHER DATA\n",
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------\n",
      " Formatted Date           | 2006-04-01 00:00:... \n",
      " Summary                  | Partly Cloudy        \n",
      " Precip Type              | rain                 \n",
      " Temperature (C)          | 9.472222222222221    \n",
      " Apparent Temperature (C) | 7.3888888888888875   \n",
      " Humidity                 | 0.89                 \n",
      " Wind Speed (km/h)        | 14.1197              \n",
      " Wind Bearing (degrees)   | 251.0                \n",
      " Visibility (km)          | 15.826300000000002   \n",
      " Loud Cover               | 0.0                  \n",
      " Pressure (millibars)     | 1015.13              \n",
      " Daily Summary            | Partly cloudy thr... \n",
      "-RECORD 1----------------------------------------\n",
      " Formatted Date           | 2006-04-01 01:00:... \n",
      " Summary                  | Partly Cloudy        \n",
      " Precip Type              | rain                 \n",
      " Temperature (C)          | 9.355555555555558    \n",
      " Apparent Temperature (C) | 7.227777777777776    \n",
      " Humidity                 | 0.86                 \n",
      " Wind Speed (km/h)        | 14.2646              \n",
      " Wind Bearing (degrees)   | 259.0                \n",
      " Visibility (km)          | 15.826300000000002   \n",
      " Loud Cover               | 0.0                  \n",
      " Pressure (millibars)     | 1015.63              \n",
      " Daily Summary            | Partly cloudy thr... \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of State of Atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Summary|count|\n",
      "+--------------------+-----+\n",
      "|              Breezy|   54|\n",
      "|Humid and Mostly ...|   40|\n",
      "|  Windy and Overcast|   45|\n",
      "|               Foggy| 7148|\n",
      "|Humid and Partly ...|   17|\n",
      "|     Windy and Foggy|    4|\n",
      "|Breezy and Partly...|  386|\n",
      "|                 Dry|   34|\n",
      "|       Partly Cloudy|31733|\n",
      "|               Clear|10890|\n",
      "|       Mostly Cloudy|28094|\n",
      "|    Breezy and Foggy|   35|\n",
      "| Breezy and Overcast|  528|\n",
      "|Dangerously Windy...|    1|\n",
      "|Breezy and Mostly...|  516|\n",
      "|Windy and Partly ...|   67|\n",
      "|               Windy|    8|\n",
      "|Dry and Partly Cl...|   86|\n",
      "|Windy and Mostly ...|   35|\n",
      "|            Overcast|16597|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Summary\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Summary| Temp|\n",
      "+-------------+-----+\n",
      "|Partly Cloudy|9.472|\n",
      "|Partly Cloudy|9.356|\n",
      "|Mostly Cloudy|9.378|\n",
      "|Partly Cloudy|8.289|\n",
      "|Mostly Cloudy|8.756|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.select(\"Summary\", F.round(\"Temperature (C)\", 3).alias(\"Temp\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Formatted Date|\n",
      "+--------------------+\n",
      "|2006-08-17 15:00:...|\n",
      "|2006-08-23 19:00:...|\n",
      "|2006-12-27 14:00:...|\n",
      "|2006-12-27 15:00:...|\n",
      "|2006-07-15 19:00:...|\n",
      "|2006-07-16 16:00:...|\n",
      "|2006-07-17 10:00:...|\n",
      "|2006-07-19 01:00:...|\n",
      "|2006-07-04 12:00:...|\n",
      "|2006-05-13 23:00:...|\n",
      "|2006-10-12 13:00:...|\n",
      "|2007-04-09 16:00:...|\n",
      "|2007-07-14 21:00:...|\n",
      "|2007-06-22 12:00:...|\n",
      "|2007-06-23 09:00:...|\n",
      "+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the fist 15-entries where visibility above 15\n",
    "\n",
    "df.select(F.col(\"Formatted Date\")).where(F.col(\"Visibility (km)\") > 15).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|Precip Type|    Temperature (C)|\n",
      "+-----------+-------------------+\n",
      "|       snow|-10.305555555555555|\n",
      "|       snow|-11.083333333333334|\n",
      "|       snow|-10.805555555555555|\n",
      "|       snow|-11.822222222222223|\n",
      "|       snow|-11.855555555555556|\n",
      "+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    ".select(\"Precip Type\", \"Temperature (C)\")\n",
    ".filter(F.col(\"Precip Type\") == \"snow\")\n",
    ".filter(F.col(\"Temperature (C)\") <= -10)\n",
    ".show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Summary|count|\n",
      "+--------------------+-----+\n",
      "|       Partly Cloudy|31733|\n",
      "|       Mostly Cloudy|28094|\n",
      "|            Overcast|16597|\n",
      "|               Clear|10890|\n",
      "|               Foggy| 7148|\n",
      "| Breezy and Overcast|  528|\n",
      "|Breezy and Mostly...|  516|\n",
      "|Breezy and Partly...|  386|\n",
      "+--------------------+-----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .groupBy(\"Summary\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .show(8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\atlas-weather\\atlas.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=33'>34</a>\u001b[0m     save_dataset(outcome)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=34'>35</a>\u001b[0m     \u001b[39m#spark.stop()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=36'>37</a>\u001b[0m main()\n",
      "\u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\atlas-weather\\atlas.ipynb Cell 12'\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=30'>31</a>\u001b[0m     spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mWeather-Atlas2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=31'>32</a>\u001b[0m     df \u001b[39m=\u001b[39m extract_dataset(spark)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/23/Desktop/start-pyspark/atlas-weather/atlas.ipynb#ch0000011?line=32'>33</a>\u001b[0m     outcome \u001b[39m=\u001b[39m transform_dataset(df)\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=225'>226</a>\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=226'>227</a>\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=227'>228</a>\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=228'>229</a>\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=229'>230</a>\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/sql/session.py?line=230'>231</a>\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=389'>390</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=390'>391</a>\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=391'>392</a>\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=392'>393</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=138'>139</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=139'>140</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=140'>141</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=141'>142</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=143'>144</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=144'>145</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=146'>147</a>\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=336'>337</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=337'>338</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=338'>339</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=339'>340</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/context.py?line=341'>342</a>\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\23\\Desktop\\start-pyspark\\env\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=104'>105</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=107'>108</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=109'>110</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    <a href='file:///c%3A/Users/23/Desktop/start-pyspark/env/lib/site-packages/pyspark/java_gateway.py?line=110'>111</a>\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.types as t\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "def extract_dataset(spark: SparkSession) -> DataFrame:\n",
    "    url = \"../dataset/weatherHistory.csv\"\n",
    "    return spark.read.option(\"header\", 'true').csv(url)\n",
    "\n",
    "\n",
    "def transform_dataset(df: DataFrame) -> DataFrame:\n",
    "    outcome = (\n",
    "        df\n",
    "        .groupBy(\"Summary\")\n",
    "        .agg(\n",
    "            f.count(\"Summary\").alias(\"count\"),\n",
    "            f.min(\"Apparent Temperature (C)\").alias(\"min_temp\"),\n",
    "            f.max(\"Apparent Temperature (C)\").alias(\"max_temp\")\n",
    "        )\n",
    "        .orderBy(f.col(\"count\").desc())\n",
    "    ).show()\n",
    "\n",
    "    return outcome\n",
    "\n",
    "\n",
    "def save_dataset(df: DataFrame) -> None:\n",
    "    df.coalesce(4).write.mode(\"overwrite\").format(\"json\").save(\"outcome.json\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"Weather-Atlas2\").getOrCreate()\n",
    "    df = extract_dataset(spark)\n",
    "    outcome = transform_dataset(df)\n",
    "    save_dataset(outcome)\n",
    "    #spark.stop()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92db4a3e6462b9fadf0a0f8c62912623cebd408e910f4b1d99e22ed15ce59dca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
